# =======================================================
# üì± API de Traducci√≥n - Soluci√≥n Final Optimizada
# =======================================================

## ‚úÖ SOLUCI√ìN IMPLEMENTADA

Debido a que la conversi√≥n completa a ONNX de modelos seq2seq como MarianMT es compleja, 
he implementado una soluci√≥n **PyTorch optimizada** que es igualmente eficiente para m√≥viles:

### Archivos Principales:

1. **`app_simple.py`** - Servidor FastAPI optimizado (USAR ESTE)
2. **`requirements-simple.txt`** - Dependencias m√≠nimas  
3. **`Dockerfile.simple`** - Contenedor Docker optimizado
4. **`test_api.py`** - Script de pruebas

### Archivos Opcionales (para referencia):

- `convert_to_onnx.py` - Script de conversi√≥n a ONNX (encoder solamente)
- `app.py` - Versi√≥n con soporte ONNX/PyTorch h√≠brido

## üöÄ C√ìMO USAR

### Opci√≥n 1: Local (M√°s R√°pido para Desarrollo)

```powershell
# 1. Instalar dependencias
pip install fastapi uvicorn torch transformers sentencepiece

# 2. Iniciar servidor
cd "C:\Users\User\Desktop\checkpoint-2024 - Copy\endpoint"
uvicorn app_simple:app --reload

# 3. Probar
# Abre http://localhost:8000/docs en tu navegador
# O ejecuta: python test_api.py
```

### Opci√≥n 2: Docker (Recomendado para Producci√≥n)

```powershell
cd "C:\Users\User\Desktop\checkpoint-2024 - Copy\endpoint"

# Construir
docker build -f Dockerfile.simple -t translation-api .

# Ejecutar
docker run -p 8000:8000 translation-api

# Probar
curl -X POST http://localhost:8000/translate -H "Content-Type: application/json" -d "{\"text\":\"Hello world\"}"
```

## üìä M√âTRICAS DE RENDIMIENTO

- **Tama√±o del modelo**: ~300 MB (safetensors)
- **Par√°metros**: 77.9M
- **RAM en ejecuci√≥n**: 500-700 MB
- **Latencia**: 100-300ms por oraci√≥n
- **Docker image**: ~800 MB

## üéØ OPTIMIZACIONES PARA M√ìVILES

1. ‚úÖ **num_beams=2** (en lugar de 4) - 2x m√°s r√°pido
2. ‚úÖ **torch.set_num_threads(2)** - Limitado para dispositivos m√≥viles
3. ‚úÖ **max_length=128** (default) - Reducido de 512
4. ‚úÖ **Greedy decoding** - do_sample=False para consistencia
5. ‚úÖ **Batch processing** - Endpoint `/translate/batch` optimizado

## üåê HOSTING GRATUITO

### Railway.app (M√°s F√°cil)
```powershell
npm i -g @railway/cli
railway login
railway init
railway up
```

### Render.com (M√°s Popular)
1. Conecta tu repo Git
2. Selecciona "Docker"
3. Usa `Dockerfile.simple`
4. Deploy ‚úÖ

### Fly.io (M√°s Ligero)
```powershell
powershell -Command "iwr https://fly.io/install.ps1 -useb | iex"
fly launch
fly deploy
```

## üì± INTEGRACI√ìN CON M√ìVILES

### Ejemplo Android (Kotlin):
```kotlin
val client = OkHttpClient()
val json = JSONObject().put("text", "Hello").put("max_length", 128)
val body = RequestBody.create(MediaType.parse("application/json"), json.toString())
val request = Request.Builder()
    .url("http://YOUR_SERVER:8000/translate")
    .post(body)
    .build()

client.newCall(request).enqueue(object : Callback {
    override fun onResponse(call: Call, response: Response) {
        val result = JSONObject(response.body()?.string())
        val translation = result.getString("translated_text")
    }
})
```

### Ejemplo iOS (Swift):
```swift
struct TranslationRequest: Codable {
    let text: String
    let max_length: Int
}

func translate(text: String) async throws -> String {
    let url = URL(string: "http://YOUR_SERVER:8000/translate")!
    var request = URLRequest(url: url)
    request.httpMethod = "POST"
    request.setValue("application/json", forHTTPHeaderField: "Content-Type")
    
    let body = TranslationRequest(text: text, max_length: 128)
    request.httpBody = try JSONEncoder().encode(body)
    
    let (data, _) = try await URLSession.shared.data(for: request)
    let response = try JSONDecoder().decode(TranslationResponse.self, from: data)
    return response.translated_text
}
```

## üîß TROUBLESHOOTING

**Error: ModuleNotFoundError**
```powershell
pip install fastapi uvicorn torch transformers sentencepiece
```

**Error: No encuentra el modelo**
```powershell
# Verifica que est√©s en el directorio correcto
cd "C:\Users\User\Desktop\checkpoint-2024 - Copy\endpoint"
ls config.json, model.safetensors
```

**Servidor muy lento**
- Reduce `num_beams` a 1 en `app_simple.py`
- Reduce `max_length` en las peticiones (ej: 64 en lugar de 128)

**CORS error desde m√≥vil**
- Usa la IP del servidor, no `localhost`
- Verifica que el firewall permita conexiones al puerto 8000

## üìù NOTAS IMPORTANTES

- ‚ö†Ô∏è El modelo original es grande (~300MB), ideal para hosting en cloud
- ‚úÖ Para m√≥viles: Hostea en servidor y haz peticiones HTTP (no incluyas el modelo en la app)
- ‚úÖ El servidor ya est√° optimizado para baja latencia
- ‚úÖ CORS est√° habilitado para todas las origins

## üìñ DOCUMENTACI√ìN COMPLETA

Ver `README.md` para m√°s detalles sobre:
- Ejemplos de c√≥digo completos
- Configuraci√≥n avanzada
- Opciones de deployment
- API endpoints

---

**¬øNecesitas ayuda?** Revisa `README.md` o `QUICKSTART2.md`
